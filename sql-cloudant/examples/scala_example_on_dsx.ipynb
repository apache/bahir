{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scala example using Spark SQL over Cloudant as a source\n",
    "\n",
    "This sample notebook is written in Scala and expects the Scala 2.11 runtime with Spark 2.1 or later.\n",
    "\n",
    "The data source for this example can be found at: http://examples.cloudant.com/crimes/\n",
    "\n",
    "Replicate the database into your own Cloudant account before you execute this script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Work with the Spark Context\n",
    "\n",
    "A Spark Context handle `sc` is available with every notebook create in the Spark Service. Use it to understand the Spark version used, the environment settings, and create a Spark SQL Context object off of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2.1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Work with a Cloudant database\n",
    "\n",
    "A Dataframe object can be created directly from a Cloudant database. To configure the database as source, pass these options:\n",
    "\n",
    "1 - package name that provides the classes (like `CloudantDataSource`) implemented in the connector to extend `BaseRelation`. For the Cloudant Spark connector this will be `org.apache.bahir.cloudant`\n",
    "\n",
    "2 - `cloudant.host` parameter to pass the Cloudant account name\n",
    "\n",
    "3 - `cloudant.user` parameter to pass the Cloudant user name\n",
    "\n",
    "4 - `cloudant.password` parameter to pass the Cloudant account password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 0:====================================================>     (9 + 1) / 10]"
     ]
    }
   ],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "val df = sqlContext.read.format(\"org.apache.bahir.cloudant\").\n",
    "option(\"cloudant.host\",\"USERNAME.cloudant.com\").\n",
    "option(\"cloudant.username\",\"USERNAME\").\n",
    "option(\"cloudant.password\",\"PASSWORD\").\n",
    "load(\"crimes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Work with a Dataframe\n",
    "\n",
    "At this point all transformations and functions should behave as specified with Spark SQL. (http://spark.apache.org/sql/)\n",
    "\n",
    "If you have any feedback or encounter an issue with the sql-cloudant connector, please open an issue (https://issues.apache.org/jira/secure/Dashboard.jspa) in Apacheâ€™s JIRA issue tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- _rev: string (nullable = true)\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- properties: struct (nullable = true)\n",
      " |    |-- compnos: string (nullable = true)\n",
      " |    |-- domestic: boolean (nullable = true)\n",
      " |    |-- fromdate: long (nullable = true)\n",
      " |    |-- main_crimecode: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- naturecode: string (nullable = true)\n",
      " |    |-- reptdistrict: string (nullable = true)\n",
      " |    |-- shooting: boolean (nullable = true)\n",
      " |    |-- source: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1:==============================================>           (8 + 2) / 10]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "269"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|naturecode|\n",
      "+----------+\n",
      "|    DISTRB|\n",
      "|       EDP|\n",
      "|    ARREST|\n",
      "|        AB|\n",
      "|      CD14|\n",
      "|    UNKEMS|\n",
      "|      REQP|\n",
      "|       EDP|\n",
      "|       MVA|\n",
      "|     IVPER|\n",
      "|      NIDV|\n",
      "|        AB|\n",
      "|    IVPREM|\n",
      "|     IVPER|\n",
      "|     IVPER|\n",
      "|       MVA|\n",
      "|      CD11|\n",
      "|    LARCEN|\n",
      "|       MVA|\n",
      "|    ARREST|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"properties.naturecode\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 5:============================================>              (3 + 1) / 4]+--------------------+--------------------+--------------------+--------------------+-------+\n",
      "|                 _id|                _rev|            geometry|          properties|   type|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------+\n",
      "|79f14b64c57461584...|1-d9518df5c255e4b...|[WrappedArray(-71...|[142035012,true,1...|Feature|\n",
      "|79f14b64c57461584...|1-798ef404b141dfb...|[WrappedArray(-71...|[142035053,false,...|Feature|\n",
      "|79f14b64c57461584...|1-08cd46894f9c579...|[WrappedArray(-71...|[142035113,false,...|Feature|\n",
      "|79f14b64c57461584...|1-be4512491803441...|[WrappedArray(-71...|[142035116,false,...|Feature|\n",
      "|79f14b64c57461584...|1-2e3e1fe35278b5d...|[WrappedArray(-71...|[142035162,false,...|Feature|\n",
      "|79f14b64c57461584...|1-e03133da93c2644...|[WrappedArray(-71...|[142035211,false,...|Feature|\n",
      "|79f14b64c57461584...|1-4c21d07bfb9f45a...|[WrappedArray(-71...|[142035316,false,...|Feature|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.col(\"properties.naturecode\").startsWith(\"DISTRB\")).show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Scala 2.11 with Spark 2.1",
   "language": "scala",
   "name": "scala-spark21"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  },
  "name": "Cloudant-spark.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
